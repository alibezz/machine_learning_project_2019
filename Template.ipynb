{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Template is created to make grading fair and straightforward. Anything not in the place as mentioned in the template would not be graded.\n",
    "\n",
    "<font color='red'> # NOTE: We would run the notebook through a Plagiarism Checker. If it is found to be copied, your work would not be graded, and the incident would be highlighted to NYU Authorities. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import csv training file given to us in the beginning of the project\n",
    "#we assume the file is in the same directory of this notebook\n",
    "df=pd.read_csv(\"qudditch_training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing values. (If ANY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop weight, finbourgh_flick, double_eight_loop due to missing information \n",
    "#and drop player_id due to not related to the target\n",
    "df.drop([\"weight\",\"finbourgh_flick\", \"double_eight_loop\",\"player_id\"], axis=1,inplace=True)\n",
    "\n",
    "#handling missing values by creating another category named 'U'\n",
    "columns_replace=[\"house\",\"player_code\",\"move_specialty\"]\n",
    "for column in columns_replace:\n",
    "\tdf[column].replace(\"?\",\"U\",inplace=True)\n",
    "df[\"gender\"].replace(\"Unknown/Invalid\",\"U\",inplace=True)\n",
    "\n",
    "#drop category 'U' from gender\n",
    "#only very few of rows have unknown type\n",
    "df = df[df.gender != 'U']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Datatype Conversion From Numeric to categoric and Vice-versa. (If ANY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for encoding (mapping)\n",
    "def map_features(features,df,dict):\n",
    "\tfor i in features:\n",
    "\t\tdf = df.replace({i:dict})\n",
    "\n",
    "\treturn df\n",
    "\n",
    "#reducing nominal values in snitchnip and stooging\n",
    "foul_dict={'None':'none','Norm':'norm','>7':'high','>8':'high','>200':'high','>300':'high'}\n",
    "foul_columns=[\"snitchnip\",\"stooging\"]\n",
    "df=map_features(foul_columns,df,foul_dict)\n",
    "\n",
    "#generate move specialty dict for reducing nominal values\n",
    "#1 stands for with specialty, 0 stands for without specialty\n",
    "def convert_move_specialty(df):\n",
    "\tdict={}\n",
    "\tfor i in df[\"move_specialty\"]:\n",
    "\t\tif i==\"U\":\n",
    "\t\t\tdict.update({\"U\":0})\n",
    "\t\telse:\n",
    "\t\t\tdict.update({i:1})\n",
    "\treturn dict\n",
    "\n",
    "move_spec_dict=convert_move_specialty(df)\n",
    "df=map_features([\"move_specialty\"],df,move_spec_dict)\n",
    "\n",
    "#23 tactics feature, ready for conversion\n",
    "tactics_columns=[\"body_blow\",\"checking\",\"dopplebeater_defence\",\"hawkshead_attacking_formation\",\"no_hands_tackle\",\"power_play\",\"sloth_grip_roll\",\"spiral_dive\",\"starfish_and_stick\",\"twirl\",\"wronski_feint\",\"zig-zag\",\"bludger_backbeat\",\"chelmondiston_charge\",\"dionysus_dive\",\"reverse_pass\",\"parkins_pincer\",\"plumpton_pass\",\"porskoff_ploy\",\"transylvanian_tackle\",\"woollongong_shimmy\"]\n",
    "\n",
    "#make a copy of dataframe for future use(feature reduction and extraction) before encoding\n",
    "df_tactics_change=df.copy()\n",
    "\n",
    "#convert tactics\n",
    "#Steady, Up, Down to 1, No to 0\n",
    "tactics_dict={'Steady':1,'No':0,'Up':1,'Down':1}\n",
    "df=map_features(tactics_columns,df,tactics_dict)\n",
    "\n",
    "#convert gender\n",
    "#Female to 0, Male to 1\n",
    "\n",
    "ordered_satisfaction = [\"Female\",\"Male\"]\n",
    "cat_dtype = pd.api.types.CategoricalDtype(ordered_satisfaction, ordered=True)\n",
    "df[\"gender\"]=df[\"gender\"].astype(cat_dtype).cat.codes\n",
    "\n",
    "#convert snitch_caught\n",
    "#No to 0, Yes to 1\n",
    "\n",
    "ordered_satisfaction = [\"No\",\"Yes\"]\n",
    "cat_dtype = pd.api.types.CategoricalDtype(ordered_satisfaction, ordered=True)\n",
    "df[\"snitch_caught\"]=df[\"snitch_caught\"].astype(cat_dtype).cat.codes\n",
    "\n",
    "#convert change\n",
    "#No to 0,Ch to 1\n",
    "\n",
    "ordered_satisfaction = [\"No\",\"Ch\"]\n",
    "cat_dtype = pd.api.types.CategoricalDtype(ordered_satisfaction, ordered=True)\n",
    "df[\"change\"]=df[\"change\"].astype(cat_dtype).cat.codes\n",
    "\n",
    "#covert target\n",
    "#NO to 0, YES to 1\n",
    "#ignore this part when transforming test data\n",
    "\n",
    "ordered_satisfaction = [\"NO\",\"YES\"]\n",
    "cat_dtype = pd.api.types.CategoricalDtype(ordered_satisfaction, ordered=True)\n",
    "df[\"quidditch_league_player\"]=df[\"quidditch_league_player\"].astype(cat_dtype).cat.codes\n",
    "\n",
    "#one-hot encoding rest of columns\n",
    "\n",
    "df=pd.get_dummies(df, columns=[\"house\",\"foul_type_id\",\"game_move_id\",\"penalty_id\",\"player_code\",\"player_type\",\"snitchnip\",\"stooging\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Reduction or extraction. (If ANY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum num_games_satout, num_games_injured, num_games_notpartof and combine them into one feature named num_game_not_participate\n",
    "\n",
    "df[\"num_game_not_participate\"]=df.num_games_satout+df.num_games_injured+df.num_games_notpartof\n",
    "\n",
    "#sum up number of tactic changes into one feature named num_tactics_change\n",
    "\n",
    "#encoding dictionary for helping calculation\n",
    "#Up and Down count for change\n",
    "tactics_change_dict={'Steady':0,'No':0,'Up':1,'Down':1}\n",
    "\n",
    "#do encoding in the copy of dataframe, help calculation\n",
    "df_tactics_change=map_features(tactics_columns,df_tactics_change,tactics_change_dict)\n",
    "\n",
    "#initialize column filled by 0\n",
    "df[\"num_tactics_change\"]=0\n",
    "\n",
    "#define function for sum change of tactics\n",
    "def sum_change_tactics(df,df_copy,columns):\n",
    "\n",
    "\tfor i in columns:\n",
    "\n",
    "\t\tdf[\"num_tactics_change\"]+=df_copy[i]\n",
    "\n",
    "sum_change_tactics(df,df_tactics_change,tactics_columns)\n",
    "\n",
    "\n",
    "#sum up number of tactics used by each player\n",
    "#create new column named num_total_tactics\n",
    "\n",
    "df[\"num_total_tactics\"]=0\n",
    "def sum_tactics(df,columns):\n",
    "\n",
    "\tfor i in columns:\n",
    "\t\t\n",
    "\t\tdf[\"num_total_tactics\"]+=df[i]\n",
    "\n",
    "\treturn df\n",
    "\n",
    "sum_tactics(df,tactics_columns)\n",
    "\n",
    "#move target to the last column\n",
    "#ignore this part when transforming test data\n",
    "df_target=df[\"quidditch_league_player\"]\n",
    "df.drop([\"quidditch_league_player\"], axis=1,inplace=True)\n",
    "df.insert(len(df.columns),\"quidditch_league_player\", df_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Any other Pre-processing Used. (Give the name along with the code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#log transform\n",
    "\n",
    "log_transform_columns=[\"num_games_satout\",\"num_games_injured\",\"num_games_notpartof\"]\n",
    "def log_transform(df,columns):\n",
    "\n",
    "\tfor i in columns:\n",
    "\t\t#add 1 to original values to perform log transform\n",
    "\t\tdf[i]+=1\n",
    "\t\tdf[i]=df[i].apply(np.log)\n",
    "\n",
    "log_transform(df,log_transform_columns)\n",
    "\n",
    "#Standardization (v-mean)/std\n",
    "\n",
    "numeric_columns=[\"game_duration\",\"num_game_moves\",\"num_game_losses\",\"num_practice_sessions\",\"num_games_satout\",\"num_games_injured\",\"num_games_notpartof\",\"num_games_won\",\"age\",\"num_total_tactics\",\"num_game_not_participate\",\"num_tactics_change\"]\n",
    "def standardize_numeric_value(df,columns):\n",
    "\tscaler = StandardScaler()\n",
    "\tfor i in columns:\n",
    "\n",
    "\t\tdf[i]=scaler.fit_transform(df[i].values.reshape(-1,1))\n",
    "\n",
    "standardize_numeric_value(df,numeric_columns)\n",
    "\n",
    "#remove outliers\n",
    "def remove_outliers(df,columns):\n",
    "\n",
    "\tfor i in columns:\n",
    "\t\t\n",
    "\t\tdf = df[np.abs(df[i] - df[i].mean()) <= (3 * df[i].std())]\n",
    "\t\t\n",
    "remove_outliers(df,numeric_columns)\n",
    "\n",
    "#generate correlation matrix to observe\n",
    "df_corr=df.corr()\n",
    "df_corr.to_csv(\"correlation.csv\")\n",
    "\n",
    "df.to_csv(\"data_aftercleaned.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1:\n",
    "Model Name: Logistic Regression<br>\n",
    "Evaluation method and metric used Name: 5-fold cross validation and the average F1-score for both classes<br>\n",
    "Name of the Hyperparameter used: training sampling, C, class_weight, penalty, solver, max_iter=1000 (fixed)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "average accuracy for folds 0.661406468103\n",
      "average fmeasure for both classes 0.526098499947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import Counter\n",
    "from sklearn.utils import resample\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "def get_cross_validation_folds(k):\n",
    "  kf = KFold(n_splits=k, random_state=RANDOM_STATE, shuffle=True)\n",
    "  return kf\n",
    "\n",
    "def separate_features_and_target(examples, target_index, feature_indices):\n",
    "  X = np.array([element[feature_indices] for element in examples])\n",
    "  y = np.array([element[target_index] for element in examples])  \n",
    "  return X, y\n",
    "\n",
    "def undersample_training(training_examples, training_classes):\n",
    "  mcc = Counter(training_classes).most_common()[0] #lcc is the Most Common Class\n",
    "  lcc = Counter(training_classes).most_common()[-1]\n",
    "  examples_in_mcc = [elem for i, elem in enumerate(training_examples) if training_classes[i] == mcc[0]]\n",
    "  number_of_samples = len(training_examples) - len(examples_in_mcc) #we undersample to the same number of samples in the least common class\n",
    "  new_samples = resample(examples_in_mcc, n_samples=number_of_samples, random_state=RANDOM_STATE)\n",
    "  training_examples = np.concatenate((new_samples, [elem for i, elem in enumerate(training_examples) if training_classes[i] == lcc[0]]))\n",
    "  training_classes = np.concatenate(([mcc[0] for i in new_samples], [lcc[0] for i in xrange(number_of_samples)]))\n",
    "  return training_examples, training_classes\n",
    "\n",
    "def bootstrap_training(training_examples, training_classes):\n",
    "  #this is where we perform oversampling (SMOTE)\n",
    "  lcc = Counter(training_classes).most_common()[-1] #lcc is the Least Common Class\n",
    "  examples_in_lcc = [elem for i, elem in enumerate(training_examples) if training_classes[i] == lcc[0]]\n",
    "  #by doing so, you end up with the same number of samples per class  \n",
    "  number_of_samples = len(training_examples) - 2*len(examples_in_lcc) \n",
    "  new_samples = resample(examples_in_lcc, n_samples=number_of_samples, random_state=RANDOM_STATE)\n",
    "  training_examples = np.concatenate((training_examples, new_samples))\n",
    "  training_classes = np.concatenate((training_classes, [lcc[0] for i in new_samples]))\n",
    "  return training_examples, training_classes\n",
    "\n",
    "def get_fmeasure(precision, recall):\n",
    "  return (2.*precision*recall)/(precision+recall)\n",
    "\n",
    "def process_input(filename):\n",
    "  #ignoring header  \n",
    "  lines = [np.array([float(i) for i in l.strip().split(',')]) for l in open(filename, 'r').readlines()[1:]] \n",
    "  return np.array(lines)\n",
    "\n",
    "####### The functions in this cell down until here are useful for all models\n",
    "\n",
    "def logistic_regression(examples, kfolds, target_index, feature_indices, params={}, sampling=None):\n",
    "  lr = LogisticRegression(**params)\n",
    "  precs = []; recs = []; accs = []; fmeasures = []\n",
    "  for train_index, test_index in kfolds:\n",
    "    X_train, y_train = separate_features_and_target(examples[train_index], target_index, feature_indices)\n",
    "    if sampling == 'over': #bootstrap training samples in the minority class \n",
    "      X_train, y_train = bootstrap_training(X_train, y_train)\n",
    "    elif sampling == 'under': #reduce the number of samples in the majority class\n",
    "      X_train, y_train = undersample_training(X_train, y_train)\n",
    "    clf = lr.fit(X_train, y_train)\n",
    "    X_test, y_test = separate_features_and_target(examples[test_index], target_index, feature_indices)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    results = precision_recall_fscore_support(y_test, y_pred)\n",
    "    precs.append(results[0])\n",
    "    recs.append(results[1])\n",
    "    accs.append(clf.score(X_test, y_test))\n",
    "    fmeasures.append((get_fmeasure(results[0][0], results[1][0]), get_fmeasure(results[0][1], results[1][1])))\n",
    "  return accs, precs, recs, fmeasures\n",
    "\n",
    "\n",
    "#below, we assume that the dataset generated in Part I is in the same folder of this notebook\n",
    "\n",
    "examples = process_input('data_aftercleaned.csv') \n",
    "kfolds = get_cross_validation_folds(5)\n",
    "SKIP = 1 \n",
    "num_features = len(examples[0]) - 1 #one field is the target\n",
    "\n",
    "### Now, we focus on Logistic Regression\n",
    "\n",
    "best_params_logreg = {'C': 0.01, 'class_weight': None, 'penalty': 'l2', 'solver': 'sag', 'max_iter':1000}\n",
    "sampling_logreg = 'over'\n",
    "print 'LOGISTIC REGRESSION'\n",
    "accs, precs, recs, fmeasures = logistic_regression(examples, kfolds.split(examples), -1,\n",
    "                                                   np.array([i + SKIP for i in xrange(num_features - SKIP)]),\n",
    "                                                   params=best_params_logreg, sampling=sampling_logreg)\n",
    "print 'average accuracy for folds', np.mean(accs)\n",
    "mean_overall_fmeasure = (np.mean([i[0] for i in fmeasures]) + np.mean([i[1] for i in fmeasures]))/2\n",
    "print 'average fmeasure for both classes', mean_overall_fmeasure \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2:\n",
    "Model Name: Adaboost<br>\n",
    "Evaluation method and metric used Name: 5-fold cross validation and the average F1-score for both classes<br>\n",
    "Name of the Hyperparameter used: Oversampling (SMOTE), weak classifiers are Decision Trees with max_depth = 1, n_estimators, and algorithm.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADABOOST\n",
      "average accuracy for folds 0.693422251575\n",
      "average fmeasure for both classes 0.534905655781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def adaboost(examples, kfolds, target_index, feature_indices, params={}, sampling=None):\n",
    "  ab =  AdaBoostClassifier(**params)\n",
    "  precs = []; recs = []; accs = []; fmeasures = []\n",
    "  for train_index, test_index in kfolds:\n",
    "    X_train, y_train = separate_features_and_target(examples[train_index], target_index, feature_indices)\n",
    "    if sampling == 'over': #bootstrap training samples in the minority class \n",
    "      X_train, y_train = bootstrap_training(X_train, y_train)\n",
    "    elif sampling == 'under': #reduce the number of samples in the majority class\n",
    "      X_train, y_train = undersample_training(X_train, y_train)\n",
    "    clf = ab.fit(X_train, [int(i) for i in y_train])\n",
    "    X_test, y_test = separate_features_and_target(examples[test_index], target_index, feature_indices)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    results = precision_recall_fscore_support(y_test, y_pred)\n",
    "    precs.append(results[0])\n",
    "    recs.append(results[1])\n",
    "    accs.append(clf.score(X_test, y_test))\n",
    "    fmeasures.append((get_fmeasure(results[0][0], results[1][0]), get_fmeasure(results[0][1], results[1][1])))\n",
    "  return accs, precs, recs, fmeasures \n",
    "\n",
    "weak_learner = DecisionTreeClassifier(max_depth=1)\n",
    "best_params_adaboost = {'base_estimator': weak_learner, 'n_estimators': 200, 'algorithm': 'SAMME'}\n",
    "sampling_adaboost = 'over'\n",
    "print 'ADABOOST'\n",
    "accs, precs, recs, fmeasures = adaboost(examples, kfolds.split(examples), -1,\n",
    "                                                   np.array([i + SKIP for i in xrange(num_features - SKIP)]),\n",
    "                                                   params=best_params_adaboost, sampling=sampling_adaboost)\n",
    "print 'average accuracy for folds', np.mean(accs)\n",
    "mean_overall_fmeasure = (np.mean([i[0] for i in fmeasures]) + np.mean([i[1] for i in fmeasures]))/2\n",
    "print 'average fmeasure for both classes', mean_overall_fmeasure \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3:\n",
    "Model Name: Random Forest <br>\n",
    "Evaluation method and metric used Name:  5-fold cross validation and the average F1-score for both classes<br>\n",
    "Name of the Hyperparameter used: Oversampling (SMOTE), n_estimators, criterion, max_depth, and max_features<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST\n",
      "average accuracy for folds 0.82739993048\n",
      "average fmeasure for both classes 0.546336883784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def random_forest(examples, kfolds, target_index, feature_indices, params={}, sampling=None):\n",
    "  rf = RandomForestClassifier(**params)\n",
    "  precs = []; recs = []; accs = []; fmeasures = []\n",
    "  for train_index, test_index in kfolds:\n",
    "    X_train, y_train = separate_features_and_target(examples[train_index], target_index, feature_indices)\n",
    "    if sampling == 'over': \n",
    "      X_train, y_train = bootstrap_training(X_train, y_train)\n",
    "    elif sampling == 'under': \n",
    "      X_train, y_train = undersample_training(X_train, y_train)\n",
    "    clf = rf.fit(X_train, [int(i) for i in y_train])\n",
    "    X_test, y_test = separate_features_and_target(examples[test_index], target_index, feature_indices)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    results = precision_recall_fscore_support(y_test, y_pred)\n",
    "    precs.append(results[0])\n",
    "    recs.append(results[1])\n",
    "    accs.append(clf.score(X_test, y_test))\n",
    "    fmeasures.append((get_fmeasure(results[0][0], results[1][0]), get_fmeasure(results[0][1], results[1][1])))\n",
    "  return accs, precs, recs, fmeasures \n",
    "\n",
    "best_params_ranfor = {'n_estimators': 5, 'criterion': 'entropy', 'max_depth': 32, 'max_features': None}\n",
    "sampling_ranfor = 'over'\n",
    "print 'RANDOM FOREST'\n",
    "accs, precs, recs, fmeasures = random_forest(examples, kfolds.split(examples), -1,\n",
    "                                                   np.array([i + SKIP for i in xrange(num_features - SKIP)]),\n",
    "                                                   params=best_params_ranfor, sampling=sampling_ranfor)\n",
    "print 'average accuracy for folds', np.mean(accs)\n",
    "mean_overall_fmeasure = (np.mean([i[0] for i in fmeasures]) + np.mean([i[1] for i in fmeasures]))/2\n",
    "print 'average fmeasure for both classes', mean_overall_fmeasure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART III: Best Hypothesis:\n",
    "Model Name: Random Forest <br>\n",
    "Reason: Highest F-measure (F1 Score) and highest accuracy. <br>\n",
    "Hyper-parameter Value: n_estimators = 5, criterion = 'entropy', max_depth = 32, max_features = None<br>\n",
    "<br>\n",
    "Here, we add code that (1) reads one training and one test file without targets, (2) performs data processing on both files, and (3) generates predictions for the examples in the test file, saving them to disk. We assume that the training and the test files are in the same directory. Please change their names if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_generic_input(filename):\n",
    "  #assumes that file has a header\n",
    "  lines = open(filename, 'r').readlines()\n",
    "  examples = [np.array([float(i) for i in l.strip().split(',')]) for l in lines[1:]]\n",
    "  header = lines[0]\n",
    "  features = header.strip().split(',')\n",
    "  return np.array(examples), np.array(features)\n",
    "\n",
    "def determine_feature_intersection(features_training, features_test):\n",
    "  #there is no guarantee that the features in training and test are \n",
    "  #the same after processing, so we force our code to work with an\n",
    "  #intersection  \n",
    "  training_dict = {}\n",
    "  test_dict = {}\n",
    "  for index, elem in enumerate(features_training):\n",
    "    training_dict[elem] = index\n",
    "  for index, elem in enumerate(features_test):\n",
    "    test_dict[elem] = index\n",
    "  key_intersection = list(set(training_dict.keys()) & set(test_dict.keys()))\n",
    "  training_indices = []\n",
    "  test_indices = []\n",
    "  for relevant_key in key_intersection:\n",
    "    training_indices.append(training_dict[relevant_key])\n",
    "    test_indices.append(test_dict[relevant_key])\n",
    "  return sorted(training_indices), sorted(test_indices)\n",
    "\n",
    "def random_forest_generic(examples_training, examples_test, target_index, \n",
    "                          features_training, features_test, params={}, sampling=None):\n",
    "  rf = RandomForestClassifier(**params)\n",
    "  precs = []; recs = []; accs = []; fmeasures = []\n",
    "  X_train, y_train = separate_features_and_target(examples_training, target_index, features_training)\n",
    "  #bootstrap training samples in the minority class \n",
    "  X_train, y_train = bootstrap_training(X_train, y_train)\n",
    "  clf = rf.fit(X_train, y_train)\n",
    "  X_test = np.array([element[features_test] for element in examples_test])\n",
    "  y_pred = clf.predict(X_test)\n",
    "  with open('test_outputs.csv', 'w') as f:\n",
    "    f.write('id_num,quidditch_league_player\\n')\n",
    "    for index, elem in enumerate(y_pred):\n",
    "      if int(elem) == 0:\n",
    "        f.write(str(index+1) + str(',') + 'NO\\n')\n",
    "      else:\n",
    "        f.write(str(index+1) + str(',') + 'YES\\n')\n",
    "\n",
    "\n",
    "#FIXME change code below to process it all directly from raw data\n",
    "#ASSUMING CLEANED TRAINING AND TEST DATA AVAILABLE, FOLLOWING PART I\n",
    "examples_training, features_training = process_generic_input(\"data_aftercleaned.csv\")\n",
    "examples_test, features_test = process_generic_input(\"test_data_aftercleaned.csv\")\n",
    "relevant_training_ids, relevant_test_ids = determine_feature_intersection(features_training, features_test)\n",
    "best_sampling = 'over'\n",
    "best_params_ranfor = {'n_estimators': 5, 'criterion': 'entropy', 'max_depth': 32, 'max_features': None}\n",
    "random_forest_generic(examples_training, examples_test, -1, relevant_training_ids, relevant_test_ids, \n",
    "                      params=best_params_ranfor, sampling=best_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
